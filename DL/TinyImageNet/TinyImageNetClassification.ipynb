{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97DbWRBzrZg7"
   },
   "source": [
    "## Домашнее задание 2. Tiny ImageNet Challenge (10 баллов + 1 бонус)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cf4_alpVICOR"
   },
   "source": [
    "В этом задании Вам предстоит обучить свёрточную нейросеть для решения задачи мультиклассовой классификации на датасете [Tiny ImageNet](https://www.kaggle.com/c/tiny-imagenet) (200 классов, по 500 изображений на класс в трейне и по 50 в валидации и тесте).\n",
    "\n",
    "``Ссылка на наше соревнование:`` [тык](https://www.kaggle.com/t/b801ab030059413e8961b10dd86b4822).\n",
    "\n",
    "## Критерии оценки\n",
    "\n",
    "* Без отчёта с графиками лосса и метрики (``accuracy@1``) на обучении работа **не принимается!**\n",
    "* Используйте **интерактивные** (не изобретайте велосипед с помощью `plt.plot`) инструменты для просмотра прогресса, например, TensorBoard или Wandb.   \n",
    "    *В Wandb также можно писать отчёты по вашим данным, попробуйте, это очень экономит время.*\n",
    "\n",
    "* Баллы выставляются на основе Private leaderboard\n",
    "    - $\\geq$ 0.45 - 10 баллов,\n",
    "    - $\\geq$ 0.35 - 6 баллов,\n",
    "    - $\\geq$ 0.25 - 3 балла.\n",
    "\n",
    "* За лучший результат на Private leaderboard +1 балл\n",
    "\n",
    "## Объяснение оценок\n",
    "\n",
    "* *Тест*: это часть набора данных, идентичная валидации, но лейблы известны только нам.\n",
    "* *Как отправить*:\n",
    "   * Не меняйте этот ноутбук, ваш код должен отработать в нём для корректной проверки инференса. Обучать можно как угодно, например, в нём же с флагом `DO_TRAIN=True`, в своём ноутбуке или из консоли.\n",
    "   * После того, как вы обучили свою сеть, [сохраните веса](https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html) в «*checkpoint.pth*» с помощью `model.state_dict()` и ` torch.save()`.\n",
    "   * Установите `DO_TRAIN = False`, нажмите «Перезапустить и запустить все ячейки» и убедитесь, что точность проверки на валидации рассчитана правильно. **Учитывайте, что вам нужен чекпоинт модели**.\n",
    "   * Загрузите «*checkpoint.pth*» на Google Диск, скопируйте на него ссылку, доступную только для просмотра, и вставьте ее также в «*solution.py*».\n",
    "\n",
    "* *Отчет*: PDF, свободная форма (можно написать в Markdown или .ipynb, главное сконвертировать в PDF в конце; отчёт в Wandb просто присылайте ссылкой), следует упомянуть:\n",
    "   * Ваша история настроек и улучшений. Как вы начинали, что искали. (*Я проанализировал те и эти документы/источники/репорты/статьи. Я попробовал то и это, чтобы адаптировать их к моей задаче. ...*)\n",
    "   * Какие архитектуры вы пробовали? Какие из них не сработали и почему, по вашему мнению? Какую выбрали на финальный сабмит и почему?\n",
    "   * То же самое касается метода обучения (batch size, алгоритм оптимизации, количество итераций...): что и почему?\n",
    "   * То же самое касается методов предотвращения переобучения (регуляризации). Какие из них вы пробовали? Каковы были их последствия и можете ли вы объяснить, почему?\n",
    "   * **Самое главное**: вы получили глубокие знания. Можете ли вы отрефлексировать и привести несколько примеров того, как опыт этого упражнения повлияет на ваше обучение будущих нейронных сетей? (хитрости, эвристики, выводы, наблюдения)\n",
    "   * **Перечислите и сошлитесь на все внешние источники кода, если вы их использовали**.\n",
    "* *Инструмент логгирования*: дополните отчет скриншотами графиков точности и лосса (на трейне и на валидации) с течением времени.\n",
    "\n",
    "## Можно:\n",
    "\n",
    "* Писать свои модели\n",
    "* Использовать готовые реализации архитектур\n",
    "* Использовать дополнительные данные \n",
    "\n",
    "## Нельзя:\n",
    "\n",
    "* Переиспользовать любые предобученные веса\n",
    "* Увеличивать изображения (например, не изменять их размер до $224 \\times 224$ или $256 \\times 256$).\n",
    "* Делиться сабмитами\n",
    "\n",
    "## Советы\n",
    "\n",
    "* **Одно изменение за раз**: не тестируйте несколько новых вещей одновременно (если вы не очень уверены, что они будут работать). Обучите модель, внесите одно изменение, обучите снова.\n",
    "* Много гуглите: постарайтесь изобрести как можно меньше велосипедов. Черпайте вдохновение из туториалов PyTorch, GitHub, блогов...\n",
    "* Используйте сверточные архитектуры.\n",
    "* Используйте графический процессор.\n",
    "* Регуляризация очень важна: L2, batch norm, early stopping, аугментации, семплирование...\n",
    "* Уделяйте большое внимание графикам точности и потерь (например, в wandb). Отслеживайте неудачи как можно раньше, прекращайте неудачные эксперименты как можно раньше.\n",
    "* 2-3 часов обучения (в Colab) должно быть достаточно для большинства моделей, возможно, 4-6 часов, если вы экспериментируете.\n",
    "* Время от времени сохраняйте чекпоинты вместе со стейтом оптимизатора на случай, если что-то пойдет не так (оптимизация расходится, Colab отключается...).\n",
    "* Не используйте слишком большие батчи, они могут работать медленно и требовать много памяти. Это справедливо и для инференса.\n",
    "* Также не забудьте использовать `torch.no_grad()` и `.eval()` во время инференса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6xx9I-VKIQ9"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import DataParallel\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import models\n",
    "from torch.optim import AdamW, SGD\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_global_seed(seed: int) -> None:\n",
    "    \"\"\"Set global seed for reproducibility.\n",
    "    :param int seed: Seed to be set\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "set_global_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HwtzExIlg9gW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps:0\n"
     ]
    }
   ],
   "source": [
    "# If `True`, will train the model from scratch and validate it.\n",
    "# If `False`, instead of training will load weights from './checkpoint.pth'.\n",
    "# When grading, we will test both cases.\n",
    "DO_TRAIN = False\n",
    "\n",
    "root_datasets = \"./\"\n",
    "\n",
    "device = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu') #Скорректирован код для подключения графических ядер Apple Silicon\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Устанавливаем гиперпараметры\n",
    "learning_rate = 1e-4\n",
    "batch_size = 512\n",
    "epochs = 30\n",
    "optimizer_type = 'adamw'\n",
    "weight_decay = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(path, kind):\n",
    "    \"\"\"\n",
    "    Return dataloader for a `kind` split of Tiny ImageNet.\n",
    "    If `kind` is 'val' or 'test', the dataloader should be deterministic.\n",
    "    path:\n",
    "        `str`\n",
    "        Path to the dataset root - a directory which contains 'train' and 'val' folders.\n",
    "    kind:\n",
    "        `str`\n",
    "        'train', 'val' or 'test'\n",
    "\n",
    "    return:\n",
    "    dataloader:\n",
    "        `torch.utils.data.DataLoader` or an object with equivalent interface\n",
    "        For each batch, should yield a tuple `(preprocessed_images, labels)` where\n",
    "        `preprocessed_images` is a proper input for `predict()` and `labels` is a\n",
    "        `torch.int64` tensor of shape `(batch_size,)` with ground truth class labels.\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    IMAGE_NET_MEAN = np.array([0.485, 0.456, 0.406])\n",
    "    IMAGE_NET_STD  = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(64, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGE_NET_MEAN, IMAGE_NET_STD),\n",
    "    ])\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(IMAGE_NET_MEAN, IMAGE_NET_STD)\n",
    "    ])\n",
    "    \n",
    "    if kind == 'train':\n",
    "        dataset    = ImageFolder(f\"{path}/train/\", transform=train_transform)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        \n",
    "        return dataloader\n",
    "    \n",
    "    \n",
    "    dataset    = ImageFolder(f\"{path}/{kind}/\", transform=val_transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param.data, 0)\n",
    "\n",
    "'''\n",
    "class TinyImageNetModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TinyImageNetModel, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(512, 200)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "'''\n",
    "\n",
    "'''class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1, bias=False)\n",
    "        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Сжимаем по пространственным измерениям\n",
    "        y = nn.AdaptiveAvgPool2d(1)(x)\n",
    "        y = self.fc1(y)\n",
    "        y = nn.ReLU()(y)\n",
    "        y = self.fc2(y)\n",
    "        y = self.sigmoid(y)\n",
    "        return x * y\n",
    "'''\n",
    "\n",
    "def add_bn_se_block(module, channels):\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        return nn.Sequential(\n",
    "            module,\n",
    "            nn.BatchNorm2d(channels),\n",
    "            SEBlock(channels)\n",
    "        )\n",
    "    return module\n",
    "\n",
    "def get_model():\n",
    "    model = models.resnet18(pretrained=False)  # Используем ResNet-18 без предобученных весов\n",
    "    \n",
    "    # Добавляем слои BatchNorm2d и SE-блоки после каждого сверточного слоя\n",
    "    def modify_layer(layer):\n",
    "        return nn.Sequential(*[add_bn_se_block(m, m.out_channels) if isinstance(m, nn.Conv2d) else m for m in layer])\n",
    "\n",
    "    model.conv1 = add_bn_se_block(model.conv1, model.conv1.out_channels)\n",
    "    model.layer1 = modify_layer(model.layer1)\n",
    "    model.layer2 = modify_layer(model.layer2)\n",
    "    model.layer3 = modify_layer(model.layer3)\n",
    "    model.layer4 = modify_layer(model.layer4)\n",
    "\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(0.10),\n",
    "        nn.Linear(model.fc.in_features, 200)\n",
    "    )\n",
    "\n",
    "    model.apply(weight_init)\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_optimizer(model, optimizer_type='adamw'):\n",
    "    \"\"\"\n",
    "    Create an optimizer object for `model`, tuned for `train_on_tinyimagenet()`.\n",
    "\n",
    "    return:\n",
    "    optimizer:\n",
    "        `torch.optim.Optimizer`\n",
    "    \"\"\"\n",
    "    if optimizer_type == 'adamw':\n",
    "        optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    elif optimizer_type == 'sgd':\n",
    "        optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optimizer type. Choose 'adamw' or 'sgd'.\")\n",
    "    \n",
    "    return optimizer\n",
    "\n",
    "def load_weights(model, checkpoint_path):\n",
    "    \"\"\"\n",
    "    Initialize `model`'s weights from `checkpoint_path` file.\n",
    "\n",
    "    model:\n",
    "        `torch.nn.Module`\n",
    "        See `get_model()`.\n",
    "    checkpoint_path:\n",
    "        `str`\n",
    "        Path to the checkpoint.\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=\"mps\")\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "AFqnb1-EFgGj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Initialize dataloaders\n",
    "train_dataloader = get_dataloader(f\"{root_datasets}/tiny-imagenet-200/\", 'train')\n",
    "val_dataloader   = get_dataloader(f\"{root_datasets}/tiny-imagenet-200/\", 'val')\n",
    "test_dataloader  = get_dataloader(f\"{root_datasets}/tiny-imagenet-200/\", 'test')\n",
    "\n",
    "# Initialize the raw model\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict(model, batch):\n",
    "    \"\"\"\n",
    "    model:\n",
    "        `torch.nn.Module`\n",
    "        The neural net, as defined by `get_model()`.\n",
    "    batch:\n",
    "        unspecified\n",
    "        A batch of Tiny ImageNet images, as yielded by `get_dataloader(..., 'val')`\n",
    "        (with same preprocessing and device).\n",
    "\n",
    "    return:\n",
    "    prediction:\n",
    "        `torch.tensor`, shape == (N, 200), dtype == `torch.float32`\n",
    "        The scores of each input image to belong to each of the dataset classes.\n",
    "        Namely, `prediction[i, j]` is the score of `i`-th minibatch sample to\n",
    "        belong to `j`-th class.\n",
    "        These scores can be 0..1 probabilities, but for better numerical stability\n",
    "        they can also be raw class scores after the last (usually linear) layer,\n",
    "        i.e. BEFORE softmax.\n",
    "    \"\"\"\n",
    "    X = batch[0].to(device)\n",
    "    out = model(X)\n",
    "    return torch.argmax(out, 1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    accuracy, loss, count = 0, 0, 0\n",
    "\n",
    "    for X, y in dataloader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        out = model(X)\n",
    "        loss += loss_fn(out, y).item()\n",
    "        accuracy += torch.sum(torch.argmax(out, 1) == y).item()\n",
    "        count += X.shape[0]\n",
    "\n",
    "    return loss / count, accuracy / count\n",
    "\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
    "\n",
    "def train_on_tinyimagenet(train_dataloader, val_dataloader, model, optimizer_type=optimizer_type, patience=8, epochs=epochs):\n",
    "    model.to(device)\n",
    "    loss_fn = LabelSmoothingLoss(classes=200, smoothing=0.1)\n",
    "\n",
    "    # Получаем оптимизатор\n",
    "    optimizer = get_optimizer(model, optimizer_type)\n",
    "\n",
    "    # Инициализация планировщика\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-2, steps_per_epoch=len(train_dataloader), epochs=epochs)\n",
    "\n",
    "    wandb.init(project=\"TinyImageNet-baseline\")\n",
    "\n",
    "    global_step = 0\n",
    "    best_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    try:\n",
    "        eval_loss = None  # Инициализация eval_loss внутри блока try\n",
    "        best_f1 = 0\n",
    "\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            model.train()\n",
    "            epoch_loss = 0\n",
    "            epoch_accuracy = 0\n",
    "            count = 0\n",
    "\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "\n",
    "            for X, y in train_dataloader:\n",
    "                global_step += 1\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                out = model(X)\n",
    "                loss = loss_fn(out, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                acc = torch.sum(torch.argmax(out, 1) == y).item()\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_accuracy += acc\n",
    "                count += y.shape[0]\n",
    "\n",
    "                # Сохраняем предсказания и истинные метки для вычисления метрик\n",
    "                all_preds.extend(torch.argmax(out, 1).cpu().numpy())\n",
    "                all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "                wandb.log({\"train/loss\": loss.item(), \"train/accuracy\": acc / y.shape[0]}, step=global_step)\n",
    "\n",
    "            # Средние значения потерь и точности за эпоху\n",
    "            avg_loss = epoch_loss / count\n",
    "            avg_accuracy = epoch_accuracy / count\n",
    "\n",
    "            eval_loss, eval_acc = validate(dataloader=val_dataloader, model=model, loss_fn=loss_fn)\n",
    "            wandb.log({\"eval/loss\": eval_loss, \"eval/accuracy\": eval_acc}, step=global_step)\n",
    "\n",
    "            # Вычисление дополнительных метрик\n",
    "            f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "            precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "            recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "            # Логирование дополнительных метрик\n",
    "            wandb.log({\"train/f1_score\": f1, \"train/precision\": precision, \"train/recall\": recall}, step=global_step)\n",
    "\n",
    "            # Обновление планировщика\n",
    "            scheduler.step(eval_loss)\n",
    "\n",
    "            # Проверка на улучшение\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                epochs_without_improvement = 0\n",
    "                # Сохраняем модель только один раз\n",
    "                torch.save({\n",
    "                    \"model\": model.state_dict(),\n",
    "                    \"optimizer\": optimizer.state_dict()\n",
    "                }, \"checkpoint.pth\")\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "\n",
    "            # Проверка на раннюю остановку\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs without improvement.\")\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        if eval_loss is None:\n",
    "            print(f\"An error occurred during training before eval_loss was defined: {e}\")\n",
    "        else:\n",
    "            print(f\"An error occurred during training: {e}\")\n",
    "    finally:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "HfZnRPEnTkqs"
   },
   "outputs": [],
   "source": [
    "if DO_TRAIN:\n",
    "    # Train from scratch\n",
    "    optimizer = get_optimizer(model)\n",
    "    train_on_tinyimagenet(train_dataloader, val_dataloader, model)\n",
    "else:\n",
    "    # Finally load weights\n",
    "    load_weights(model, \"./checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): SEBlock(\n",
       "      (fc1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (fc2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Dropout(p=0.1, inplace=False)\n",
       "    (1): Linear(in_features=512, out_features=200, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "fdsdJ3kVg9gX"
   },
   "outputs": [],
   "source": [
    "example_batch, example_batch_labels = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "CDJw8MokFxP9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class / Ground truth class\n",
      "107 / 000\n",
      "071 / 000\n",
      "122 / 000\n",
      "147 / 000\n",
      "063 / 000\n",
      "067 / 000\n",
      "061 / 000\n",
      "197 / 000\n",
      "044 / 000\n",
      "131 / 000\n",
      "063 / 000\n",
      "107 / 000\n",
      "106 / 000\n",
      "011 / 000\n",
      "040 / 000\n"
     ]
    }
   ],
   "source": [
    "# Classify some validation samples\n",
    "example_batch, example_batch_labels = next(iter(val_dataloader))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    example_predicted_labels = predict(model, [example_batch])\n",
    "\n",
    "print(\"Predicted class / Ground truth class\")\n",
    "for predicted, gt in list(zip(example_predicted_labels, example_batch_labels))[:15]:\n",
    "    print(\"{:03d} / {:03d}\".format(predicted, gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.016173881149291993, 0.0038)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(val_dataloader, model, loss_fn=LabelSmoothingLoss(classes=200, smoothing=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "U_Qddecy7-uS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.38%\n"
     ]
    }
   ],
   "source": [
    "# Print validation accuracy\n",
    "val_loss, val_accuracy =  validate(val_dataloader, model, loss_fn=LabelSmoothingLoss(classes=200, smoothing=0.1))\n",
    "val_accuracy *= 100\n",
    "\n",
    "print(\"Validation accuracy: %.2f%%\" % val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "wUS1v3dug9gY"
   },
   "outputs": [],
   "source": [
    "map_classes = {class_idx: class_name for class_name, class_idx in train_dataloader.dataset.class_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "dWGm6cFrg9gY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:04<00:00,  4.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# example real submission\n",
    "pred_dict = {}\n",
    "pred_labels = []\n",
    "model.eval()\n",
    "\n",
    "for batch, _ in tqdm(test_dataloader):\n",
    "    with torch.no_grad():\n",
    "        predicted_labels = predict(model, [batch])\n",
    "    pred_labels.extend(predicted_labels.tolist())\n",
    "for i, (img_name, _) in enumerate(test_dataloader.dataset.imgs):\n",
    "    pred_dict[img_name.split(\"/\")[-1]] = map_classes[pred_labels[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "fKJIf9nXg9gY"
   },
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame(pred_dict.items(), columns=[\"id\", \"pred\"])\n",
    "submission_df.to_csv(\"baseline_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чекпоинт: https://drive.google.com/file/d/1kyDk_tof56U1FN2uAuLNGSyeO2yrMiLM/view?usp=sharing"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
